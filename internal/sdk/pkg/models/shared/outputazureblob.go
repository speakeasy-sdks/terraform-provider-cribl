// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"fmt"
)

// OutputAzureBlobAuthenticationMethod - Enter connection string directly, or select a stored secret
type OutputAzureBlobAuthenticationMethod string

const (
	OutputAzureBlobAuthenticationMethodSecret OutputAzureBlobAuthenticationMethod = "secret"
	OutputAzureBlobAuthenticationMethodManual OutputAzureBlobAuthenticationMethod = "manual"
)

func (e OutputAzureBlobAuthenticationMethod) ToPointer() *OutputAzureBlobAuthenticationMethod {
	return &e
}

func (e *OutputAzureBlobAuthenticationMethod) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "secret":
		fallthrough
	case "manual":
		*e = OutputAzureBlobAuthenticationMethod(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureBlobAuthenticationMethod: %v", v)
	}
}

// OutputAzureBlobCompress - Choose data compression format to apply before moving files to final destination.
type OutputAzureBlobCompress string

const (
	OutputAzureBlobCompressNone OutputAzureBlobCompress = "none"
	OutputAzureBlobCompressGzip OutputAzureBlobCompress = "gzip"
)

func (e OutputAzureBlobCompress) ToPointer() *OutputAzureBlobCompress {
	return &e
}

func (e *OutputAzureBlobCompress) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = OutputAzureBlobCompress(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureBlobCompress: %v", v)
	}
}

// OutputAzureBlobDataFormat - Format of the output data.
type OutputAzureBlobDataFormat string

const (
	OutputAzureBlobDataFormatParquet OutputAzureBlobDataFormat = "parquet"
	OutputAzureBlobDataFormatRaw     OutputAzureBlobDataFormat = "raw"
	OutputAzureBlobDataFormatJSON    OutputAzureBlobDataFormat = "json"
)

func (e OutputAzureBlobDataFormat) ToPointer() *OutputAzureBlobDataFormat {
	return &e
}

func (e *OutputAzureBlobDataFormat) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "parquet":
		fallthrough
	case "raw":
		fallthrough
	case "json":
		*e = OutputAzureBlobDataFormat(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureBlobDataFormat: %v", v)
	}
}

// OutputAzureBlobBackpressureBehavior - Whether to block or drop events when all receivers are exerting backpressure.
type OutputAzureBlobBackpressureBehavior string

const (
	OutputAzureBlobBackpressureBehaviorBlock OutputAzureBlobBackpressureBehavior = "block"
	OutputAzureBlobBackpressureBehaviorDrop  OutputAzureBlobBackpressureBehavior = "drop"
)

func (e OutputAzureBlobBackpressureBehavior) ToPointer() *OutputAzureBlobBackpressureBehavior {
	return &e
}

func (e *OutputAzureBlobBackpressureBehavior) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "block":
		fallthrough
	case "drop":
		*e = OutputAzureBlobBackpressureBehavior(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureBlobBackpressureBehavior: %v", v)
	}
}

// OutputAzureBlobDataPageVersion - Serialization format of data pages. Note that not all reader implentations support Data page V2.
type OutputAzureBlobDataPageVersion string

const (
	OutputAzureBlobDataPageVersionDataPageV1 OutputAzureBlobDataPageVersion = "DATA_PAGE_V1"
	OutputAzureBlobDataPageVersionDataPageV2 OutputAzureBlobDataPageVersion = "DATA_PAGE_V2"
)

func (e OutputAzureBlobDataPageVersion) ToPointer() *OutputAzureBlobDataPageVersion {
	return &e
}

func (e *OutputAzureBlobDataPageVersion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "DATA_PAGE_V1":
		fallthrough
	case "DATA_PAGE_V2":
		*e = OutputAzureBlobDataPageVersion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureBlobDataPageVersion: %v", v)
	}
}

// OutputAzureBlobParquetVersion - Determines which data types are supported and how they are represented.
type OutputAzureBlobParquetVersion string

const (
	OutputAzureBlobParquetVersionParquet10 OutputAzureBlobParquetVersion = "PARQUET_1_0"
	OutputAzureBlobParquetVersionParquet24 OutputAzureBlobParquetVersion = "PARQUET_2_4"
	OutputAzureBlobParquetVersionParquet26 OutputAzureBlobParquetVersion = "PARQUET_2_6"
)

func (e OutputAzureBlobParquetVersion) ToPointer() *OutputAzureBlobParquetVersion {
	return &e
}

func (e *OutputAzureBlobParquetVersion) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "PARQUET_1_0":
		fallthrough
	case "PARQUET_2_4":
		fallthrough
	case "PARQUET_2_6":
		*e = OutputAzureBlobParquetVersion(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureBlobParquetVersion: %v", v)
	}
}

type OutputAzureBlobType string

const (
	OutputAzureBlobTypeAzureBlob OutputAzureBlobType = "azure_blob"
)

func (e OutputAzureBlobType) ToPointer() *OutputAzureBlobType {
	return &e
}

func (e *OutputAzureBlobType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "azure_blob":
		*e = OutputAzureBlobType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for OutputAzureBlobType: %v", v)
	}
}

type OutputAzureBlob struct {
	// Append output's ID to staging location.
	AddIDToStagePath *bool `json:"addIdToStagePath,omitempty"`
	// Enter connection string directly, or select a stored secret
	AuthType *OutputAzureBlobAuthenticationMethod `json:"authType,omitempty"`
	// JavaScript expression to define the output filename prefix (can be constant).
	BaseFileName *string `json:"baseFileName,omitempty"`
	// Choose data compression format to apply before moving files to final destination.
	Compress *OutputAzureBlobCompress `json:"compress,omitempty"`
	// Enter your Azure Storage account connection string. If left blank, Stream will fall back to env.AZURE_STORAGE_CONNECTION_STRING.
	ConnectionString *string `json:"connectionString,omitempty"`
	// A container organizes a set of blobs, similar to a directory in a file system. Value can be a JavaScript expression enclosed in quotes or backticks. @{product} evaluates the expression at init time. The expression can evaluate to a constant value, and can reference Global Variables, e.g., `myContainer-${C.env["CRIBL_WORKER_ID"]}`
	ContainerName string `json:"containerName"`
	// Creates the configured container in Azure Blob Storage if it does not already exist.
	CreateContainer *bool `json:"createContainer,omitempty"`
	// Root directory prepended to path before uploading. Value can be a JavaScript expression enclosed in quotes or backticks. @{product} evaluates the expression at init time. The expression can evaluate to a constant value, and can reference Global Variables, e.g., `myBlobPrefix-${C.env["CRIBL_WORKER_ID"]}`
	DestPath string `json:"destPath"`
	// How often (secs) to clean-up empty directories when 'Remove Staging Dirs' is enabled.
	EmptyDirCleanupSec *int64 `json:"emptyDirCleanupSec,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// JavaScript expression to define the output filename suffix (can be constant).  The `__format` variable refers to the value of the `Data format` field (`json` or `raw`).  The `__compression` field refers to the kind of compression being used (`none` or `gzip`)
	FileNameSuffix *string `json:"fileNameSuffix,omitempty"`
	// Format of the output data.
	Format *OutputAzureBlobDataFormat `json:"format,omitempty"`
	// Unique ID for this output
	ID *string `json:"id,omitempty"`
	// Maximum amount of time to keep inactive files open. Files open for longer than this will be closed and moved to final output location.
	MaxFileIdleTimeSec *int64 `json:"maxFileIdleTimeSec,omitempty"`
	// Maximum amount of time to write to a file. Files open for longer than this will be closed and moved to final output location.
	MaxFileOpenTimeSec *int64 `json:"maxFileOpenTimeSec,omitempty"`
	// Maximum uncompressed output file size. Files of this size will be closed and moved to final output location.
	MaxFileSizeMB *int64 `json:"maxFileSizeMB,omitempty"`
	// Maximum number of files to keep open concurrently. When exceeded, @{product} will close the oldest open files and move them to the final output location.
	MaxOpenFiles *int64 `json:"maxOpenFiles,omitempty"`
	// Whether to block or drop events when all receivers are exerting backpressure.
	OnBackpressure *OutputAzureBlobBackpressureBehavior `json:"onBackpressure,omitempty"`
	// Serialization format of data pages. Note that not all reader implentations support Data page V2.
	ParquetDataPageVersion *OutputAzureBlobDataPageVersion `json:"parquetDataPageVersion,omitempty"`
	// Ideal memory size for page segments. E.g., 1MB or 128MB. Generally, lower values improve reading speed, while higher values improve compression. Imposes a target, not a strict limit; the final size of a row group may be larger or smaller.
	ParquetPageSize *string `json:"parquetPageSize,omitempty"`
	// Ideal memory size for row group segments. E.g., 128MB or 1GB. Affects memory use when writing. Imposes a target, not a strict limit; the final size of a row group may be larger or smaller.
	ParquetRowGroupSize *string `json:"parquetRowGroupSize,omitempty"`
	// Determines which data types are supported and how they are represented.
	ParquetVersion *OutputAzureBlobParquetVersion `json:"parquetVersion,omitempty"`
	// JS expression defining how files are partitioned and organized. Default is date-based. If blank, Stream will fall back to the event's __partition field value – if present – otherwise to each location's root directory.
	PartitionExpr *string `json:"partitionExpr,omitempty"`
	// Pipeline to process data before sending out to this output.
	Pipeline *string `json:"pipeline,omitempty"`
	// Remove empty staging directories after moving files.
	RemoveEmptyDirs *bool `json:"removeEmptyDirs,omitempty"`
	// To log rows that @{product} skips due to data mismatch, first set logging to Debug, then toggle this on. Logs up to 20 unique rows.
	ShouldLogInvalidRows *bool   `json:"shouldLogInvalidRows,omitempty"`
	Spacer               *string `json:"spacer,omitempty"`
	// Filesystem location in which to buffer files, before compressing and moving to final destination. Use performant stable storage.
	StagePath string `json:"stagePath"`
	// Add tags for filtering and grouping in @{product}.
	Streamtags []string `json:"streamtags,omitempty"`
	// Set of fields to automatically add to events using this output. E.g.: cribl_pipe, c*. Wildcards supported.
	SystemFields []string `json:"systemFields,omitempty"`
	// Select (or create) a stored text secret
	TextSecret *string              `json:"textSecret,omitempty"`
	Type       *OutputAzureBlobType `json:"type,omitempty"`
}
