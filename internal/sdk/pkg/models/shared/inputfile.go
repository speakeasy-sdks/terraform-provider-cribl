// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"fmt"
)

type InputFileConnections struct {
	// Select a Destination.
	Output string `json:"output"`
	// Select Pipeline or Pack. Optional.
	Pipeline *string `json:"pipeline,omitempty"`
}

type InputFileMetadata struct {
	// Field name
	Name string `json:"name"`
	// JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)
	Value string `json:"value"`
}

// InputFileMode - Choose how to discover files to monitor.
type InputFileMode string

const (
	InputFileModeManual InputFileMode = "manual"
)

func (e InputFileMode) ToPointer() *InputFileMode {
	return &e
}

func (e *InputFileMode) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "manual":
		*e = InputFileMode(v)
		return nil
	default:
		return fmt.Errorf("invalid value for InputFileMode: %v", v)
	}
}

type InputFileOptionalFieldsInGeneralSection string

const (
	InputFileOptionalFieldsInGeneralSectionMode  InputFileOptionalFieldsInGeneralSection = "mode"
	InputFileOptionalFieldsInGeneralSectionDepth InputFileOptionalFieldsInGeneralSection = "depth"
)

func (e InputFileOptionalFieldsInGeneralSection) ToPointer() *InputFileOptionalFieldsInGeneralSection {
	return &e
}

func (e *InputFileOptionalFieldsInGeneralSection) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "mode":
		fallthrough
	case "depth":
		*e = InputFileOptionalFieldsInGeneralSection(v)
		return nil
	default:
		return fmt.Errorf("invalid value for InputFileOptionalFieldsInGeneralSection: %v", v)
	}
}

// InputFilePqCompression - Codec to use to compress the persisted data.
type InputFilePqCompression string

const (
	InputFilePqCompressionNone InputFilePqCompression = "none"
	InputFilePqCompressionGzip InputFilePqCompression = "gzip"
)

func (e InputFilePqCompression) ToPointer() *InputFilePqCompression {
	return &e
}

func (e *InputFilePqCompression) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = InputFilePqCompression(v)
		return nil
	default:
		return fmt.Errorf("invalid value for InputFilePqCompression: %v", v)
	}
}

// InputFilePqMode - With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.
type InputFilePqMode string

const (
	InputFilePqModeSmart  InputFilePqMode = "smart"
	InputFilePqModeAlways InputFilePqMode = "always"
)

func (e InputFilePqMode) ToPointer() *InputFilePqMode {
	return &e
}

func (e *InputFilePqMode) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "smart":
		fallthrough
	case "always":
		*e = InputFilePqMode(v)
		return nil
	default:
		return fmt.Errorf("invalid value for InputFilePqMode: %v", v)
	}
}

type InputFilePq struct {
	// The number of events to send downstream before committing that Stream has read them.
	CommitFrequency *int64 `json:"commitFrequency,omitempty"`
	// Codec to use to compress the persisted data.
	Compress *InputFilePqCompression `json:"compress,omitempty"`
	// The maximum number of events to hold in memory before writing the events to disk.
	MaxBufferSize *int64 `json:"maxBufferSize,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	MaxFileSize *string `json:"maxFileSize,omitempty"`
	// The maximum amount of disk space the queue is allowed to consume. Once reached, the system stops queueing and applies the fallback Queue-full behavior. Enter a numeral with units of KB, MB, etc.
	MaxSize *string `json:"maxSize,omitempty"`
	// With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.
	Mode *InputFilePqMode `json:"mode,omitempty"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>.
	Path *string `json:"path,omitempty"`
}

type InputFileInputType string

const (
	InputFileInputTypeSplunk            InputFileInputType = "splunk"
	InputFileInputTypeSplunkHec         InputFileInputType = "splunk_hec"
	InputFileInputTypeSyslog            InputFileInputType = "syslog"
	InputFileInputTypeTcpjson           InputFileInputType = "tcpjson"
	InputFileInputTypeGrafana           InputFileInputType = "grafana"
	InputFileInputTypeLoki              InputFileInputType = "loki"
	InputFileInputTypeHTTP              InputFileInputType = "http"
	InputFileInputTypeHTTPRaw           InputFileInputType = "http_raw"
	InputFileInputTypeFirehose          InputFileInputType = "firehose"
	InputFileInputTypeElastic           InputFileInputType = "elastic"
	InputFileInputTypeKafka             InputFileInputType = "kafka"
	InputFileInputTypeConfluentCloud    InputFileInputType = "confluent_cloud"
	InputFileInputTypeMsk               InputFileInputType = "msk"
	InputFileInputTypeKinesis           InputFileInputType = "kinesis"
	InputFileInputTypeEventhub          InputFileInputType = "eventhub"
	InputFileInputTypeAzureBlob         InputFileInputType = "azure_blob"
	InputFileInputTypeMetrics           InputFileInputType = "metrics"
	InputFileInputTypeSqs               InputFileInputType = "sqs"
	InputFileInputTypeS3                InputFileInputType = "s3"
	InputFileInputTypeSnmp              InputFileInputType = "snmp"
	InputFileInputTypeCrowdstrike       InputFileInputType = "crowdstrike"
	InputFileInputTypeTCP               InputFileInputType = "tcp"
	InputFileInputTypeRawUDP            InputFileInputType = "raw_udp"
	InputFileInputTypeOffice365Service  InputFileInputType = "office365_service"
	InputFileInputTypeOffice365Mgmt     InputFileInputType = "office365_mgmt"
	InputFileInputTypeOffice365MsgTrace InputFileInputType = "office365_msg_trace"
	InputFileInputTypePrometheus        InputFileInputType = "prometheus"
	InputFileInputTypeEdgePrometheus    InputFileInputType = "edge_prometheus"
	InputFileInputTypePrometheusRw      InputFileInputType = "prometheus_rw"
	InputFileInputTypeAppscope          InputFileInputType = "appscope"
	InputFileInputTypeGooglePubsub      InputFileInputType = "google_pubsub"
	InputFileInputTypeOpenTelemetry     InputFileInputType = "open_telemetry"
	InputFileInputTypeDatadogAgent      InputFileInputType = "datadog_agent"
	InputFileInputTypeWef               InputFileInputType = "wef"
	InputFileInputTypeDatagen           InputFileInputType = "datagen"
	InputFileInputTypeCribl             InputFileInputType = "cribl"
	InputFileInputTypeCriblmetrics      InputFileInputType = "criblmetrics"
	InputFileInputTypeCriblHTTP         InputFileInputType = "cribl_http"
	InputFileInputTypeCriblTCP          InputFileInputType = "cribl_tcp"
	InputFileInputTypeWinEventLogs      InputFileInputType = "win_event_logs"
	InputFileInputTypeSystemMetrics     InputFileInputType = "system_metrics"
	InputFileInputTypeWindowsMetrics    InputFileInputType = "windows_metrics"
	InputFileInputTypeSystemState       InputFileInputType = "system_state"
	InputFileInputTypeKubeMetrics       InputFileInputType = "kube_metrics"
	InputFileInputTypeKubeLogs          InputFileInputType = "kube_logs"
	InputFileInputTypeKubeEvents        InputFileInputType = "kube_events"
	InputFileInputTypeExec              InputFileInputType = "exec"
	InputFileInputTypeSplunkSearch      InputFileInputType = "splunk_search"
	InputFileInputTypeFile              InputFileInputType = "file"
	InputFileInputTypeJournalFiles      InputFileInputType = "journal_files"
)

func (e InputFileInputType) ToPointer() *InputFileInputType {
	return &e
}

func (e *InputFileInputType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "splunk":
		fallthrough
	case "splunk_hec":
		fallthrough
	case "syslog":
		fallthrough
	case "tcpjson":
		fallthrough
	case "grafana":
		fallthrough
	case "loki":
		fallthrough
	case "http":
		fallthrough
	case "http_raw":
		fallthrough
	case "firehose":
		fallthrough
	case "elastic":
		fallthrough
	case "kafka":
		fallthrough
	case "confluent_cloud":
		fallthrough
	case "msk":
		fallthrough
	case "kinesis":
		fallthrough
	case "eventhub":
		fallthrough
	case "azure_blob":
		fallthrough
	case "metrics":
		fallthrough
	case "sqs":
		fallthrough
	case "s3":
		fallthrough
	case "snmp":
		fallthrough
	case "crowdstrike":
		fallthrough
	case "tcp":
		fallthrough
	case "raw_udp":
		fallthrough
	case "office365_service":
		fallthrough
	case "office365_mgmt":
		fallthrough
	case "office365_msg_trace":
		fallthrough
	case "prometheus":
		fallthrough
	case "edge_prometheus":
		fallthrough
	case "prometheus_rw":
		fallthrough
	case "appscope":
		fallthrough
	case "google_pubsub":
		fallthrough
	case "open_telemetry":
		fallthrough
	case "datadog_agent":
		fallthrough
	case "wef":
		fallthrough
	case "datagen":
		fallthrough
	case "cribl":
		fallthrough
	case "criblmetrics":
		fallthrough
	case "cribl_http":
		fallthrough
	case "cribl_tcp":
		fallthrough
	case "win_event_logs":
		fallthrough
	case "system_metrics":
		fallthrough
	case "windows_metrics":
		fallthrough
	case "system_state":
		fallthrough
	case "kube_metrics":
		fallthrough
	case "kube_logs":
		fallthrough
	case "kube_events":
		fallthrough
	case "exec":
		fallthrough
	case "splunk_search":
		fallthrough
	case "file":
		fallthrough
	case "journal_files":
		*e = InputFileInputType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for InputFileInputType: %v", v)
	}
}

type InputFile struct {
	// A list of event breaking rulesets that will be applied, in order, to the input data stream.
	BreakerRulesets []string `json:"breakerRulesets,omitempty"`
	// Skip files with modification times earlier than the maximum age duration.
	CheckFileModTime *bool `json:"checkFileModTime,omitempty"`
	// Direct connections to Destinations, optionally via a Pipeline or a Pack.
	Connections []InputFileConnections `json:"connections,omitempty"`
	// Set how many subdirectories deep to search. Use 0 to search only files in the given path, 1 to also look in its immediate subdirectories, etc. Leave it empty for unlimited depth.
	Depth *int64 `json:"depth,omitempty"`
	// Enable/disable this input
	Disabled *bool `json:"disabled,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// The full path of discovered files are matched against this wildcard list.
	Filenames []string `json:"filenames,omitempty"`
	// Unique ID for this input
	ID string `json:"id"`
	// Time, in seconds, before an idle file is closed.
	IdleTimeout *int64 `json:"idleTimeout,omitempty"`
	// Time, in seconds, between scanning for files.
	Interval *int64 `json:"interval,omitempty"`
	// The maximum file age, in duration form (e.g,: 60s, 4h, 3d, 1w), for files to monitor. Age will be relative to file modification time. Default of no value will apply no max age filters.
	MaxAgeDur *string `json:"maxAgeDur,omitempty"`
	// Fields to add to events from this input.
	Metadata []InputFileMetadata `json:"metadata,omitempty"`
	// Choose how to discover files to monitor.
	Mode                           *InputFileMode                           `json:"mode,omitempty"`
	OptionalFieldsInGeneralSection *InputFileOptionalFieldsInGeneralSection `json:"optionalFieldsInGeneralSection,omitempty"`
	// Directory path to search for files. Environment variables will be resolved, e.g. $CRIBL_HOME/log/.
	Path *string `json:"path,omitempty"`
	// Pipeline to process data from this Source before sending it through the Routes.
	Pipeline *string      `json:"pipeline,omitempty"`
	Pq       *InputFilePq `json:"pq,omitempty"`
	// For details on Persistent Queues, see: [https://docs.cribl.io/stream/persistent-queues](https://docs.cribl.io/stream/persistent-queues)
	PqEnabled *bool `json:"pqEnabled,omitempty"`
	// Select whether to send data to Routes, or directly to Destinations.
	SendToRoutes *bool `json:"sendToRoutes,omitempty"`
	// The amount of time (in milliseconds) the Event Breaker will wait for new data to be sent to a specific channel, before flushing the data stream out, as-is, to the Pipelines.
	StaleChannelFlushMs *int64 `json:"staleChannelFlushMs,omitempty"`
	// Add tags for filtering and grouping in @{product}.
	Streamtags []string `json:"streamtags,omitempty"`
	// Read only new entries at the end of all files discovered at next startup. @{product} will then read newly discovered files from the head. Disable this to resume reading all files from head.
	TailOnly *bool              `json:"tailOnly,omitempty"`
	Type     InputFileInputType `json:"type"`
}
