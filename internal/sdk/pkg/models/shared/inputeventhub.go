// Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT.

package shared

import (
	"encoding/json"
	"fmt"
)

type InputEventhubConnections struct {
	// Select a Destination.
	Output string `json:"output"`
	// Select Pipeline or Pack. Optional.
	Pipeline *string `json:"pipeline,omitempty"`
}

type InputEventhubMetadata struct {
	// Field name
	Name string `json:"name"`
	// JavaScript expression to compute field's value, enclosed in quotes or backticks. (Can evaluate to a constant.)
	Value string `json:"value"`
}

// InputEventhubPqCompression - Codec to use to compress the persisted data.
type InputEventhubPqCompression string

const (
	InputEventhubPqCompressionNone InputEventhubPqCompression = "none"
	InputEventhubPqCompressionGzip InputEventhubPqCompression = "gzip"
)

func (e InputEventhubPqCompression) ToPointer() *InputEventhubPqCompression {
	return &e
}

func (e *InputEventhubPqCompression) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "none":
		fallthrough
	case "gzip":
		*e = InputEventhubPqCompression(v)
		return nil
	default:
		return fmt.Errorf("invalid value for InputEventhubPqCompression: %v", v)
	}
}

// InputEventhubPqMode - With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.
type InputEventhubPqMode string

const (
	InputEventhubPqModeSmart  InputEventhubPqMode = "smart"
	InputEventhubPqModeAlways InputEventhubPqMode = "always"
)

func (e InputEventhubPqMode) ToPointer() *InputEventhubPqMode {
	return &e
}

func (e *InputEventhubPqMode) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "smart":
		fallthrough
	case "always":
		*e = InputEventhubPqMode(v)
		return nil
	default:
		return fmt.Errorf("invalid value for InputEventhubPqMode: %v", v)
	}
}

type InputEventhubPq struct {
	// The number of events to send downstream before committing that Stream has read them.
	CommitFrequency *int64 `json:"commitFrequency,omitempty"`
	// Codec to use to compress the persisted data.
	Compress *InputEventhubPqCompression `json:"compress,omitempty"`
	// The maximum number of events to hold in memory before writing the events to disk.
	MaxBufferSize *int64 `json:"maxBufferSize,omitempty"`
	// The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.).
	MaxFileSize *string `json:"maxFileSize,omitempty"`
	// The maximum amount of disk space the queue is allowed to consume. Once reached, the system stops queueing and applies the fallback Queue-full behavior. Enter a numeral with units of KB, MB, etc.
	MaxSize *string `json:"maxSize,omitempty"`
	// With Smart mode, PQ will write events to the filesystem only when it detects backpressure from the processing engine. With Always On mode, PQ will always write events directly to the queue before forwarding them to the processing engine.
	Mode *InputEventhubPqMode `json:"mode,omitempty"`
	// The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/inputs/<input-id>.
	Path *string `json:"path,omitempty"`
}

// InputEventhubAuthenticationSASLMechanism - SASL authentication mechanism to use
type InputEventhubAuthenticationSASLMechanism string

const (
	InputEventhubAuthenticationSASLMechanismPlain       InputEventhubAuthenticationSASLMechanism = "plain"
	InputEventhubAuthenticationSASLMechanismOauthbearer InputEventhubAuthenticationSASLMechanism = "oauthbearer"
)

func (e InputEventhubAuthenticationSASLMechanism) ToPointer() *InputEventhubAuthenticationSASLMechanism {
	return &e
}

func (e *InputEventhubAuthenticationSASLMechanism) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "plain":
		fallthrough
	case "oauthbearer":
		*e = InputEventhubAuthenticationSASLMechanism(v)
		return nil
	default:
		return fmt.Errorf("invalid value for InputEventhubAuthenticationSASLMechanism: %v", v)
	}
}

// InputEventhubAuthentication - Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
type InputEventhubAuthentication struct {
	// Enable authentication.
	Disabled bool `json:"disabled"`
	// SASL authentication mechanism to use
	Mechanism *InputEventhubAuthenticationSASLMechanism `json:"mechanism,omitempty"`
}

type InputEventhubTLSSettingsClientSide struct {
	Disabled bool `json:"disabled"`
	// Reject certs that are not authorized by a CA in the CA certificate path, or by another trusted CA (e.g., the system's CA).
	RejectUnauthorized *bool `json:"rejectUnauthorized,omitempty"`
}

type InputEventhubType string

const (
	InputEventhubTypeEventhub InputEventhubType = "eventhub"
)

func (e InputEventhubType) ToPointer() *InputEventhubType {
	return &e
}

func (e *InputEventhubType) UnmarshalJSON(data []byte) error {
	var v string
	if err := json.Unmarshal(data, &v); err != nil {
		return err
	}
	switch v {
	case "eventhub":
		*e = InputEventhubType(v)
		return nil
	default:
		return fmt.Errorf("invalid value for InputEventhubType: %v", v)
	}
}

type InputEventhub struct {
	// Maximum time to wait for Kafka to respond to an authentication request
	AuthenticationTimeout *int64 `json:"authenticationTimeout,omitempty"`
	// How often to commit offsets. If both this and Offset commit threshold are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
	AutoCommitInterval *int64 `json:"autoCommitInterval,omitempty"`
	// How many events are needed to trigger an offset commit. If both this and Offset commit interval are set, @{product} commits offsets when either condition is met. If both are empty, @{product} commits offsets after each batch.
	AutoCommitThreshold *int64 `json:"autoCommitThreshold,omitempty"`
	// List of Event Hubs Kafka brokers to connect to, e.g., yourdomain.servicebus.windows.net:9093. The hostname can be found in the host portion of the primary or secondary connection string in Shared Access Policies.
	Brokers []string `json:"brokers"`
	// Maximum time to wait for a connection to complete successfully
	ConnectionTimeout *int64 `json:"connectionTimeout,omitempty"`
	// Direct connections to Destinations, optionally via a Pipeline or a Pack.
	Connections []InputEventhubConnections `json:"connections,omitempty"`
	// Enable/disable this input
	Disabled *bool `json:"disabled,omitempty"`
	// Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere.
	Environment *string `json:"environment,omitempty"`
	// Whether to start reading from earliest available data, relevant only during initial subscription.
	FromBeginning *bool `json:"fromBeginning,omitempty"`
	// Specifies the consumer group this instance belongs to, default is 'Cribl'.
	GroupID *string `json:"groupId,omitempty"`
	//       Expected time (a.k.a heartbeat.interval.ms in Kafka domain) between heartbeats to the consumer coordinator when using Kafka's group management facilities.
	//       Value must be lower than sessionTimeout, and typically should not exceed 1/3 of the sessionTimeout value.
	//       See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
	HeartbeatInterval *int64 `json:"heartbeatInterval,omitempty"`
	// Unique ID for this input
	ID *string `json:"id,omitempty"`
	// Maximum number of bytes that Kafka will return per fetch request. Defaults to 10485760 (10 MB).
	MaxBytes *int64 `json:"maxBytes,omitempty"`
	// Maximum amount of data that Kafka will return per partition, per fetch request. Must equal or exceed the maximum message size (maxBytesPerPartition) that Kafka is configured to allow. Otherwise, @{product} can get stuck trying to retrieve messages. Defaults to 1048576 (1 MB).
	MaxBytesPerPartition *int64 `json:"maxBytesPerPartition,omitempty"`
	// If messages are failing, you can set the maximum number of retries as high as 100 to prevent loss of data.
	MaxRetries *int64 `json:"maxRetries,omitempty"`
	// Fields to add to events from this input.
	Metadata []InputEventhubMetadata `json:"metadata,omitempty"`
	// Enable feature to minimize duplicate events by only starting one consumer for each topic partition.
	MinimizeDuplicates *bool `json:"minimizeDuplicates,omitempty"`
	// Pipeline to process data from this Source before sending it through the Routes.
	Pipeline *string          `json:"pipeline,omitempty"`
	Pq       *InputEventhubPq `json:"pq,omitempty"`
	// For details on Persistent Queues, see: [https://docs.cribl.io/stream/persistent-queues](https://docs.cribl.io/stream/persistent-queues)
	PqEnabled *bool `json:"pqEnabled,omitempty"`
	// Specifies a time window during which @{product} can reauthenticate if needed. Creates the window measuring backwards from the moment when credentials are set to expire.
	ReauthenticationThreshold *int64 `json:"reauthenticationThreshold,omitempty"`
	//       Maximum allowed time (a.k.a rebalance.timeout.ms in Kafka domain) for each worker to join the group after a rebalance has begun.
	//       If the timeout is exceeded, the coordinator broker will remove the worker from the group.
	//       See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
	RebalanceTimeout *int64 `json:"rebalanceTimeout,omitempty"`
	// Maximum time to wait for Kafka to respond to a request
	RequestTimeout *int64 `json:"requestTimeout,omitempty"`
	// Authentication parameters to use when connecting to brokers. Using TLS is highly recommended.
	Sasl *InputEventhubAuthentication `json:"sasl,omitempty"`
	// Select whether to send data to Routes, or directly to Destinations.
	SendToRoutes *bool `json:"sendToRoutes,omitempty"`
	//       Timeout (a.k.a session.timeout.ms in Kafka domain) used to detect client failures when using Kafka's group management facilities.
	//       If the client sends the broker no heartbeats before this timeout expires, the broker will remove this client from the group, and will initiate a rebalance.
	//       Value must be lower than rebalanceTimeout.
	//       See details [here](https://github.com/Azure/azure-event-hubs-for-kafka/blob/master/CONFIGURATION.md).
	SessionTimeout *int64 `json:"sessionTimeout,omitempty"`
	// Add tags for filtering and grouping in @{product}.
	Streamtags []string                            `json:"streamtags,omitempty"`
	TLS        *InputEventhubTLSSettingsClientSide `json:"tls,omitempty"`
	// The name of the Event Hub (a.k.a. Kafka topic) to subscribe to. Warning: To optimize performance, Cribl suggests subscribing each Event Hubs Source to only a single topic.
	Topics []string           `json:"topics"`
	Type   *InputEventhubType `json:"type,omitempty"`
}
